import os
import google.generativeai as genai
import json
import streamlit as st
# DeprecationWarningì— ë”°ë¼ langchain_community ë° langchain_huggingfaceë¡œ ì„í¬íŠ¸ ê²½ë¡œ ìˆ˜ì •
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFaceHub
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from bs4 import BeautifulSoup as bs
import requests
from transformers import pipeline

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
target_file_path = os.path.join(current_dir, '../data/APIkey.json')

# íŒŒì¼ ì½ê¸°
with open(target_file_path, 'r') as file:
    data = json.load(file)  # JSON íŒŒì¼ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë¡œë“œ

# API í‚¤ ì„¤ì •
api_key = data['Gemini']
genai.configure(api_key=api_key)

os.environ["HUGGINGFACEHUB_API_TOKEN"] = data['Huggingface']
os.environ.get("HUGGINGFACEHUB_API_TOKEN")


# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_model():
    model = genai.GenerativeModel('gemini-1.5-flash')
    print("Model loaded...")
    return model


# Streamlit ì„¸ì…˜ì—ì„œ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ì„¤ì •
if "model" not in st.session_state:
    with st.spinner("ëª¨ë¸ë¡œë”©ì¤‘"):
        st.session_state.model = load_model()
    

# DB ë° ëª¨ë¸ ì¶”ê°€ ë¶€ë¶„ ğŸ˜¤ --------------------------------------------------
# ëª¨ë¸ ìƒì„±ë¶€
# SKT í•œêµ­ì–´ ì„ë² ë”© ë° QA ëª¨ë¸ ì„¤ì •
if "embedding" not in st.session_state:
    with st.spinner("ì„ë² ë”© ë¡œë”©ì¤‘"):
        hf_embeddings = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    st.session_state.embedding = hf_embeddings

# ê²€ìƒ‰ë¶€
def load_Korean_game(gametitle, display=5, pageno=1):
    url = f'https://www.grac.or.kr/WebService/GameSearchSvc.asmx/game?display={display}&pageno={pageno}&gametitle={gametitle}'
    
    response = requests.get(url)
    response.raise_for_status()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

    # HTML íŒŒì‹± ë° ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
    soup = bs(response.text, 'html.parser')
    content = soup.find_all('item')  # item íƒœê·¸ë“¤ ì¶”ì¶œ
    
    paragraphs = []
    for idx, i in enumerate(content):
        # print(f'idx : {idx}')
        # print('-'*50)
        # print(f'content : {i}')
        # title = i.find('gametitle')  # gametitle íƒœê·¸ ì¶”ì¶œ
        if i:
            # print(f'gametitle : {i.text}')
            paragraphs.append(i.text)
    
    # í…ìŠ¤íŠ¸ ë³€í™˜ ë° Document ê°ì²´ë¡œ ë³€í™˜    
    documents = [Document(page_content=doc, metadata={"source": f"doc{i+1}"}) for i, doc in enumerate(paragraphs)]
    # text = "\n".join(paragraphs)    
    # documents = [Document(page_content=text, metadata={"source": url})]
    
    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¿
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)
    
    return splits

def create_vectorstore(splits):
    valid_splits = []
    for i, split in enumerate(splits):
        print(f"Processing split {i+1}/{len(splits)}: {split.page_content[:30]}...")  # ì²« 30ì ì¶œë ¥
        
        # ì„ë² ë”© ìƒì„± ê³¼ì • í™•ì¸
        embedding = st.session_state.embedding.embed_query(split.page_content)
        
        # ì„ë² ë”©ì´ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ ê¸¸ì´ì™€ í˜•íƒœ ì¶œë ¥
        if embedding and len(embedding) > 0:
            print(f"Embedding created for split {i+1}, length: {len(embedding)}")
            valid_splits.append(split)
        else:
            print(f"Empty embedding for split {i+1}")

    if valid_splits:
        vectorstore = Chroma.from_documents(documents=valid_splits, embedding=st.session_state.embedding )
        return vectorstore
    else:
        raise ValueError("No valid documents with embeddings to add to vector store.")
from langchain_community.vectorstores import Chroma

# KoAlpaca ëª¨ë¸ ë¡œë“œ
# qa_pipeline = pipeline(
#     "text2text-generation",
#     model="beomi/KoAlpaca-Polyglot-5.8B",
#     tokenizer="beomi/KoAlpaca-Polyglot-5.8B"
# )
# qa_pipeline = pipeline(
#     "text-generation",
#     model="beomi/KoAlpaca-Polyglot-5.8B",
#     tokenizer="beomi/KoAlpaca-Polyglot-5.8B"
# )
# qa_pipeline = pipeline(
#     "text-generation",  # ë˜ëŠ” "text2text-generation" íƒœìŠ¤í¬
#     model="t5-small",
#     tokenizer="t5-small"
# )

# T5 ê¸°ë°˜ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
if "qa_pipeline" not in st.session_state:    
    qa_pipeline = pipeline(
        "text2text-generation",
        model="t5-small",
        tokenizer="t5-small"
    )
    st.session_state.qa_pipeline = qa_pipeline
    
# ---------------------------ëª¨ë¸ìƒì„±ë¶€ì¢…ë£Œ---------------------------

# ì„¸ì…˜ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì‚¬ìš©ì ì…ë ¥ê³¼ ë²„íŠ¼ ì²˜ë¦¬
topic = st.text_input('ê²Œì„ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.')
question = st.text_input('ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.')
if st.button('ì§ˆë¬¸') and question:
    # ì‚¬ìš©ì ì…ë ¥ë¶€
    splits = load_Korean_game(topic)
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state.chat_history.append(f"[ì‚¬ìš©ì]: {question}")
    st.text(f'[ì‚¬ìš©ì]\n{question}')
    
    if splits:
        st.session_state.vectorstore = create_vectorstore(splits)
    else:
        print("No content to create vector store.")
        
        
    from transformers import pipeline
    
    # context êµ¬ì„±
    # context = " ".join([split.page_content for split in splits])
    
    
    context_doc = st.session_state.vectorstore.similarity_search(question, k=4)
    # context = context_doc[0].page_content if context_doc else "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    # context = ' '.join([doc.page_conent for doc in context_docs])
    
    prompt = f"Context: {context_doc}\nQuestion: {question}\nAnswer in a complete sentence:"
    # response = gemini_model(prompt)
    
    response = st.session_state.model.generate_content(prompt)
    answer = response.candidates[0].content.parts[0].text
    
    # # ëª¨ë¸ ì‘ë‹µ ìƒì„±
    # response = st.session_state.model.generate_content(user_querie)
    # model_response = response.candidates[0].content.parts[0].text
    # st.text(f'[ëª¨ë¸]\n{model_response}')
    
    # # ëª¨ë¸ ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ ğŸ˜¤ íˆìŠ¤í† ë¦¬ ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ëª»í•¨
    # st.session_state.chat_history.append(f"[ëª¨ë¸]: {model_response}")
    
    # contextì™€ questionì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ì „ë‹¬
    # input_text = f"{context}\n\nì§ˆë¬¸: {question}"
    # result = st.session_state.qa_pipeline(input_text, max_new_tokens=100)

    # ì§ˆë¬¸ê³¼ ë‹µë³€ë§Œ ì¶œë ¥
    st.text(f"ì§ˆë¬¸: {question}")
    st.text(f"ë‹µë³€: {answer}")
    # st.text(f"ë‹µë³€: {result[0]['generated_text']}")
    
    # ì „ì²´ íˆìŠ¤í† ë¦¬ ì¶œë ¥
    st.text('--------------------------------------------')
    st.text("\n".join(st.session_state.chat_history))
